use std::{collections::HashMap, ops::Sub, sync::Arc};

use ark_ff::FftField;
use ark_serialize::{CanonicalDeserialize, CanonicalSerialize, SerializationError};
use ark_std::rand::Rng;
use bincode::ErrorKind;
use itertools::izip;
use serde::{Deserialize, Serialize};
use stoffelmpc_network::{Message, Network, NetworkError, Node, PartyId, SessionId};
use thiserror::Error;
use tokio::{sync::Mutex, task::JoinError};

use crate::common::share::{shamir::NonRobustShamirShare, ShareError};

use super::{
    batch_recon::{batch_recon::BatchReconNode, BatchReconError},
    robust_interpolate::RobustShamirShare,
    DoubleShamirShare,
};

/// Error type for the triple generation protocol.
#[derive(Debug, Error)]
pub enum TripleGenError {
    /// Error that describes an failure in the network processes.
    #[error("network error: {0:?}")]
    NetworkError(#[from] NetworkError),
    /// Error that arises when there is a failure manipulating shares.
    #[error("share error: {0:?}")]
    ShareError(#[from] ShareError),
    /// This error arises when there is not enough random double shares in the
    /// preprocessing to complete the triple generation protocol.
    #[error("not enough preprocessing")]
    NotEnoughPreprocessing,
    /// Error during the serialization using [`bincode`].
    #[error("error during the serialization using bincode: {0:?}")]
    BincodeSerializationError(#[from] Box<ErrorKind>),
    /// Error during the serialization using [`ark_serialize`].
    #[error("error during the serialization using bincode: {0:?}")]
    ArkSerializationError(#[from] SerializationError),
    /// The error arises when there are not enough random shares in the input to the triple
    /// generation protocol.
    #[error("wrong ammount of shares")]
    NotEnoughShares,
    /// Error during the batch reconstruction protocol.
    #[error("batch reconstruction error: {0:?}")]
    BatchReconError(#[from] BatchReconError),
    /// Error during the execution of async operations.
    #[error("async error: {0:?}")]
    AsyncError(#[from] JoinError),
}

/// Represents a Beaver triple of non-robus Shamir shares.
pub struct ShamirBeaverTriple<F: FftField> {
    /// First random value of the triple.
    pub a: NonRobustShamirShare<F>,
    /// Second random value of the triple.
    pub b: NonRobustShamirShare<F>,
    /// Multiplication of both random values.
    pub mult: NonRobustShamirShare<F>,
}

impl<F> ShamirBeaverTriple<F>
where
    F: FftField,
{
    /// Creates a new Shamir Beaver triple with `a` and `b` being the random values of the triple
    /// and `mult` is the multiplication of `a` and `b`.
    pub fn new(
        a: NonRobustShamirShare<F>,
        b: NonRobustShamirShare<F>,
        mult: NonRobustShamirShare<F>,
    ) -> Self {
        Self { a, b, mult }
    }
}

/// Parameters for the Beaver triple generation protocol.
pub struct TripleGenParams {
    /// The ID of the session.
    pub session_id: SessionId,
    /// The number of parties participating in the triple generation protocol.
    pub n_parties: usize,
    /// The upper bound of corrupt parties participating in the triple generation protocol.
    pub threshold: usize,
    /// The number of triples that will be generated.
    pub n_triples: usize,
}

impl TripleGenParams {
    /// Creates a new set of parameters for the triple generation protocol.
    pub fn new(
        session_id: SessionId,
        n_parties: usize,
        threshold: usize,
        n_triples: usize,
    ) -> Self {
        Self {
            session_id,
            n_parties,
            threshold,
            n_triples,
        }
    }
}

/// Current state of the Shamir Beaver triple generation protocol.
pub enum ProtocolState {
    /// The protocol has not been initialized.
    NotInitialized,
    /// The protocol has been initialized and under execution.
    Initialized,
    /// The protocol has finished.
    Finished,
}

/// Storage necessary for the triple generation protocol.
pub struct TripleGenStorage {
    /// Current state of the protocol execution.
    pub protocol_state: ProtocolState,
}

impl TripleGenStorage {
    /// Creates an empty state for the protocol.
    pub fn empty() -> Self {
        Self {
            protocol_state: ProtocolState::NotInitialized,
        }
    }
}

/// Generic message for the triple generation protocol.
///
/// This generic message contains the payload in bytes of any message sent during the protocol
/// execution. Any message that is sent in the protocol is converted into bytes that are placed in
/// the `payload`. Once a party receives a message, it takes the payload and deserialize it to the
/// specific message sent during the protocol execution.
#[derive(Clone, Serialize, Deserialize)]
pub struct TripleGenMessage {
    /// The ID of the party.
    pub sender_id: PartyId,
    /// The session ID of the instance.
    pub session_id: SessionId,
    /// The payload of the message.
    pub payload: Vec<u8>,
}

impl TripleGenMessage {
    /// Creates a new generic message for the triple generation protocol.
    pub fn new(sender_id: PartyId, session_id: SessionId, payload: Vec<u8>) -> Self {
        Self {
            sender_id,
            session_id,
            payload,
        }
    }
}

impl Message for TripleGenMessage {
    fn sender_id(&self) -> PartyId {
        self.sender_id
    }

    fn bytes(&self) -> &[u8] {
        &self.payload
    }
}

#[derive(Debug, CanonicalSerialize, CanonicalDeserialize)]
pub struct ShareMessage<F: FftField> {
    pub sender_id: PartyId,
    pub session_id: SessionId,
    pub shares: Vec<RobustShamirShare<F>>,
}

impl<F> ShareMessage<F>
where
    F: FftField,
{
    pub fn new(
        sender_id: PartyId,
        session_id: SessionId,
        shares: Vec<RobustShamirShare<F>>,
    ) -> Self {
        Self {
            sender_id,
            session_id,
            shares,
        }
    }
}

pub struct TripleGenNode {
    pub id: PartyId,
    pub params: TripleGenParams,
    pub storage: Arc<Mutex<HashMap<SessionId, Arc<Mutex<TripleGenStorage>>>>>,
}

impl TripleGenNode {
    pub async fn get_or_create_store(
        &mut self,
        session_id: SessionId,
    ) -> Arc<Mutex<TripleGenStorage>> {
        let mut storage = self.storage.lock().await;
        storage
            .entry(session_id)
            .or_insert(Arc::new(Mutex::new(TripleGenStorage::empty())))
            .clone()
    }

    pub async fn init<F: FftField, R: Rng, N: Network>(
        &mut self,
        random_shares_a: Vec<RobustShamirShare<F>>,
        random_shares_b: Vec<RobustShamirShare<F>>,
        randousha_pairs: Vec<DoubleShamirShare<F>>,
        network: Arc<N>,
    ) -> Result<Vec<NonRobustShamirShare<F>>, TripleGenError> {
        // Validates that there are enough random double shares and random shares to perform the
        // operation.
        if randousha_pairs.len() != self.params.n_triples
            || random_shares_a.len() != self.params.n_triples
            || random_shares_b.len() != self.params.n_triples
        {
            return Err(TripleGenError::NotEnoughPreprocessing);
        }

        let mut sub_shares_deg_2t = Vec::new();
        for (share_a, share_b, ran_dou_sha) in
            izip!(&random_shares_a, &random_shares_b, &randousha_pairs)
        {
            let mult_share_deg_2t = share_a.share_mul(share_b)?;
            let sub_share_deg_2t =
                (mult_share_deg_2t - &RobustShamirShare::from(ran_dou_sha.degree_2t.clone()))?;
            sub_shares_deg_2t.push(sub_share_deg_2t);
        }

        // Call to Batch Reconstruction.
        let batch_recon_node =
            BatchReconNode::<F>::new(self.id, self.params.n_parties, self.params.threshold)?;
        batch_recon_node
            .init_batch_reconstruct(&sub_shares_deg_2t, Arc::clone(&network))
            .await?;

        let sub_values_clean = tokio::spawn(async move {
            loop {
                match batch_recon_node.secrets {
                    None => continue,
                    Some(rbc_result) => return rbc_result,
                };
            }
        })
        .await?;

        let mut result_shares = Vec::new();
        for (sub_value, pair) in sub_values_clean.into_iter().zip(randousha_pairs) {
            let result_share = (pair.degree_t + &sub_value)?;
            result_shares.push(result_share);
        }
        Ok(result_shares)
    }
}
